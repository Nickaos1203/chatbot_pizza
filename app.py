import os
import json
import requests
import chromadb
from chromadb.utils import embedding_functions
import gradio as gr

# Configuration
COLLECTION_NAME = "rag_collection"
EMBEDDING_MODEL = "mxbai-embed-large"
LLM_MODEL = "llama3.2"
CHROMA_PATH = "./chroma_db"
OLLAMA_URL = "http://localhost:11434/api/chat"

# Initialisation
os.environ["CHROMA_ENABLE_TELEMETRY"] = "false"

print("Connexion √† ChromaDB...")
client = chromadb.PersistentClient(path=CHROMA_PATH)

print("Chargement du mod√®le d'embedding via Ollama...")
ollama_ef = embedding_functions.OllamaEmbeddingFunction(
    url="http://localhost:11434/api/embeddings",
    model_name=EMBEDDING_MODEL,
)

print(f"Chargement de la collection : {COLLECTION_NAME}")
collection = client.get_or_create_collection(
    name=COLLECTION_NAME,
    embedding_function=ollama_ef
)

# Fonction RAG
def rag_chatbot_llm(query, top_k=3):
    if not query.strip():
        return "Veuillez entrer une question."

    print(f"üîç Recherche des {top_k} chunks les plus pertinents...")
    results = collection.query(
        query_texts=[query],
        n_results=top_k,
        include=["documents", "metadatas"]
    )

    documents = results.get("documents", [[]])[0]

    if not documents:
        return "Aucun r√©sultat trouv√©."

    # Construction du contexte √† partir des chunks r√©cup√©r√©s
    context = "\n".join(documents)

    # Construction du prompt pour le LLM
    prompt = (
        "En te basant uniquement sur le contexte suivant, r√©ponds de mani√®re claire et compl√®te √† la question.\n\n"
        f"--- CONTEXTE ---\n{context}\n\n"
        f"--- QUESTION ---\n{query}"
    )

    # Appel au mod√®le LLM via Ollama
    try:
        response = requests.post(OLLAMA_URL, json={
            "model": LLM_MODEL,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False
        })

        if response.status_code == 200:
            return json.loads(response.text)["message"]["content"]
        else:
            return f"Erreur LLM : {response.text}"

    except Exception as e:
        return f"Erreur lors de l'appel au LLM : {str(e)}"


# Interface Gradio
with gr.Blocks() as demo:
    gr.Markdown("# ü§ñ Chatbot RAG ")

    with gr.Row():
        question = gr.Textbox(label="Votre question", placeholder="Ex: Quels sont les ingr√©dients de la pizza hawa√Øenne ?")
        submit = gr.Button("R√©pondre")

    response_box = gr.Markdown(label="R√©ponse")

    submit.click(fn=rag_chatbot_llm, inputs=question, outputs=response_box)


# Lancement
if __name__ == "__main__":
    demo.launch()
